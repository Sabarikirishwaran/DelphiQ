{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11965025,"sourceType":"datasetVersion","datasetId":7523729}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-cache-dir torch torchvision torchaudio -q\n!pip install --no-cache-dir pytorch-lightning -q\n!pip install --no-cache-dir custatevec-cu12 -q\n!pip install --no-cache-dir lightning pennylane-lightning-gpu -q\n!pip install --no-cache-dir pandas matplotlib -q\n#!pip install \"jax[cuda11_pip]==0.4.28\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gzip\nimport io\nimport pandas as pd\nimport pennylane.numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\ndef load_weather_data(folder_path='.', datetime_format='%d-%b-%Y %H:%M'):\n    \"\"\"\n    Load all .csv weather files (rainfall/wind) in folder_path.        \n    \"\"\"\n    datasets = {}\n    # Expected headers\n    expected_keywords = ['date', 'time', 'rain', 'wind']\n    \n    for file in glob(os.path.join(folder_path, '*.csv')):\n        name = os.path.basename(file).replace('.csv', '')\n        try:            \n            with open(file, 'rt') as f:\n                lines = f.readlines()\n                            \n            header_idx = 0\n            for idx, line in enumerate(lines):\n                tokens = set(line.strip().lower().split(\" \"))                \n                if any(keyword in tokens for keyword in expected_keywords):\n                    header_idx = idx\n                    break                        \n            \n            # Load csv\n            df = pd.read_csv(\n                io.StringIO(''.join(lines[header_idx:])),\n                dtype=str,\n                low_memory=False\n            )\n            \n            # Parse datetime\n            if 'date' in df.columns and 'time' in df.columns:\n                df['datetime'] = pd.to_datetime(\n                    df['date'] + ' ' + df['time'],\n                    format=datetime_format,\n                    errors='coerce'\n                )\n                df.drop(columns=['date', 'time'], inplace=True)                \n            else:\n                # fallback\n                dt_col = next((col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()), None) \n                df['datetime'] = pd.to_datetime(df[dt_col], format=datetime_format, errors='coerce')                \n                df.drop(columns=[dt_col], inplace=True)\n                        \n            rename_map = {}\n            for c in df.columns:\n                lc = c.lower()\n                if 'rain' in lc: \n                    rename_map[c] = 'rainfall_mm'\n                if 'wind' in lc and 'speed' in lc:\n                    rename_map[c] = 'wind_speed_mps'\n            df.rename(columns=rename_map, inplace=True)\n            df['wind_speed_mps'] = df['wind_speed_mps'].replace({'VRB': None, 'NA': None, 'NaN': None})\n                        \n            df = df[['datetime', 'rainfall_mm', 'wind_speed_mps']].copy()            \n            df = df.dropna(subset=['datetime', 'rainfall_mm', 'wind_speed_mps'])                                \n            df['rainfall_mm'] = pd.to_numeric(df['rainfall_mm'], errors='coerce')\n            df['wind_speed_mps'] = pd.to_numeric(df['wind_speed_mps'], errors='coerce')\n            \n            df = df.sort_values('datetime').reset_index(drop=True)\n            start_date = pd.Timestamp('2019-10-21')\n            df = df[df['datetime'] >= start_date]                \n            print(f\"Weather loaded: {name} ({len(df)} rows)\")\n        \n        except Exception as e:\n            print(f\"Skipping {name}: {e}\")\n    \n    return df\n\ndef load_river_level_data(folder_path='.', river_sites=None, datetime_format='%Y-%m-%d %H:%M:%S'):\n    \"\"\"\n    Load river level csv for specified river_sites.        \n    \"\"\"\n    if river_sites is None:\n        river_sites = ['waikato', 'waipa', '']\n    datasets = {}\n    expected_cols = ['date', 'time', 'wlvalue', 'flvalue']\n    for file in glob(os.path.join(folder_path, '*.csv')) + glob(os.path.join(folder_path, '*.csv')):\n        name = os.path.basename(file)\n        \n        if not any(site.lower() in name.lower() for site in river_sites):\n            continue\n        try:            \n            opener = gzip.open if file.endswith('.gz') else open\n            with opener(file, 'rt') as f:\n                lines = f.readlines()\n            \n            header_idx = 0\n            for idx, line in enumerate(lines):\n                tokens = set(line.strip().lower().replace(',',' ').split())                \n                if any(keyword in tokens for keyword in expected_cols):\n                    header_idx = idx\n                    break\n                                \n            df = pd.read_csv(\n                io.StringIO(''.join(lines[header_idx:])),\n                dtype=str, low_memory=False\n            )            \n                    \n            if 'date' in df.columns and 'time' in df.columns:                \n                df['dt'] = df['date'] + ' ' + df['time']                                \n                df['datetime'] = pd.to_datetime(\n                    df['dt'],\n                    format=datetime_format,\n                    errors='coerce'\n                )                \n                df.drop(columns=['date', 'time', 'dt'], inplace=True)                \n                        \n            df['river_level'] = df['wlvalue'].astype(float)\n            df = df.dropna(subset=['datetime', 'river_level'])            \n            df = df[['datetime', 'river_level']].sort_values('datetime')\n            df = df.set_index('datetime')\n            #resample data hourly\n            df = df.resample('h').ffill()  \n            print(f\"River loaded: {name} ({len(df)} rows)\")\n            datasets[name] = df\n        except Exception as e:\n            print(f\"Skipping river {name}: {e}\")\n    return datasets\n\ndef merge_weather_and_river(weather_df, river_df):\n    \"\"\"\n    Merge weather and river level data on exact datetime match (inner join).    \n    \"\"\"\n    # Ensure both are in datetime64[ns] type\n    weather_df = weather_df.copy()\n    river_df = river_df.copy()    \n    \n    # Inner join on exact datetime match\n    merged = pd.merge(weather_df, river_df, on='datetime', how='inner')\n\n    #print(f\"Merged {len(merged)} rows on datetime match\")\n\n    return merged\n\ndef visualize_merged_data(data, threshold=150, title='Merged Timeseries'):\n    \"\"\"\n    Plot rainfall and river level with threshold line.\n    \"\"\"\n    fig, ax1 = plt.subplots(figsize=(12,6))\n    ax1.plot(data['datetime'], data['rainfall_mm'], label='Rainfall (mm)', color='blue')\n    ax1.set_xlabel('Datetime'); ax1.set_ylabel('Rainfall (mm)', color='blue')\n    ax1.tick_params(axis='y', labelcolor='blue')    \n    \n    ax2 = ax1.twinx()\n    ax2.plot(data['datetime'], data['river_level'], label='River Level (m)', color='green')\n    ax2.set_ylabel('River Level (m)', color='green')\n    ax2.tick_params(axis='y', labelcolor='green')\n    \n    #fig.tight_layout()\n    plt.title(title)\n    fig.legend()\n    plt.show()\n\n# Load data\nweather_data = load_weather_data(folder_path='/kaggle/input/wbc-datasets/rainfall_data/Observations_Hourly_Auckland_Aerodrome_NZAAA_1993Jan01_2025May23.csv')\nriver_data = load_river_level_data(folder_path='/kaggle/input/wbc-datasets/riverlevel_data', river_sites=['waikato','waipa'])\nmerged_data = {}\n\n# Merge matching datasets\nfor rname, rdata in river_data.items():\n    merged_data[rname] = merge_weather_and_river(weather_data, rdata)\n\nkey = next(iter(merged_data))\ndf_merge = merged_data[key]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize\nfor key, df_merge in merged_data.items():    \n    visualize_merged_data(df_merge, title=key)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(merged_data.keys())\nmerged_data['RiverLevel-WaikatoRiver-HamiltonTrafficBr-1stJan1993-23rdMay2025.csv']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport pennylane as qml\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport csv\nfrom tqdm import trange\nfrom torch.utils.data import TensorDataset, DataLoader\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nnb_reuploading = 3\nlookback = 24  # window size\nfeature_col = ['rainfall_mm','wind_speed_mps']\nX_features = len(feature_col)\ntarget_col = ['river_level']\ny_features = len(target_col)\n\nnb_qubit_reupload = X_features\nnum_variational = 3\nnb_epoch = 1\nlr = 0.01\nbatch_size = 16\n\ndev_reupload = qml.device(\"default.qubit\", wires=nb_qubit_reupload, shots=None)\n\ndef generate_windowed_dataset(data, lookback=24, feature_cols=None, target_col='river_level'):\n    \"\"\"\n    Create (X, y) with features from lookback hours    \n    \"\"\"\n    if feature_cols is None:\n        feature_cols = [c for c in ['rainfall_mm','wind_speed_mps'] if c in data.columns]\n    X, y = [], []\n    for i in range(len(data) - lookback):\n        window = data[feature_cols].iloc[i:i+lookback].values\n        target = data[target_col].iloc[i + lookback]\n        X.append(window)\n        y.append(target)\n    return np.array(X), np.array(y)\n\ndef encoding_layer(params_encoding, x, lookback, nb_qubit):        \n    x = x.reshape(-1, lookback, X_features)\n    #print(\"params_encoding.shape, x.shape\", params_encoding.shape, x.shape)\n    \n    rotation_gates = [qml.RX, qml.RY, qml.RZ]\n    for q in range(X_features):\n        for j in range(lookback):            \n            gate = rotation_gates[j % len(rotation_gates)]            \n            gate(params_encoding[q,j] * x[:,j,q], wires=q)\n\ndef variational_layer(params_variational, lookback, nb_qubit, num_variational):\n    #print(\"params_variational.shape\", params_variational.shape)\n    rotation_gates = [qml.RX, qml.RY, qml.RZ]\n    for q in range(nb_qubit):\n        for k in range(num_variational):\n            gate = rotation_gates[k % len(rotation_gates)]                        \n            gate(params_variational[q, k], wires=q)\n\n@qml.qnode(dev_reupload, interface=\"torch\", diff_method=\"backprop\")\ndef quantum_circuit_reupload(x, params):\n    params_enc, params_var, ent_wieghts = params        \n    for i in range(nb_reuploading):\n        encoding_layer(params_enc[i], x, lookback, nb_qubit_reupload)\n        variational_layer(params_var[i], lookback, nb_qubit_reupload, num_variational)\n\n    qml.StronglyEntanglingLayers(weights=ent_wieghts, wires=range(nb_qubit_reupload))    \n    #return  [qml.expval(qml.PauliZ(i)) for i in range(nb_qubit_reupload)]\n    return qml.expval(qml.PauliZ(0))\n\ndef prediction_accuracy(y_pred, y_true, tolerance=0.1):\n    correct = torch.sum(torch.abs(y_pred - y_true) < tolerance)\n    return 100 * correct / len(y_true)\n\ndef split_data(data, test_ratio, lookback, target_col):\n    X, y = generate_windowed_dataset(data, lookback=lookback, target_col=target_col)\n    split = int(len(X) * (1 - test_ratio))\n    return X[:split], y[:split], X[split:], y[split:]\n\ndef plot_metrics(loss_hist, acc_hist, grad_norm_hist, name):\n    epochs = range(1, len(loss_hist)+1)\n    plt.figure()\n    plt.plot(epochs, loss_hist, label=\"Test Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{name}: Test Loss over Epochs\")\n    plt.legend()\n    plt.savefig(f\"{name}_loss_plot.png\", dpi=300)\n    plt.close()\n\n    plt.figure()\n    plt.plot(epochs, acc_hist, label=\"Test Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"{name}: Test Accuracy over Epochs\")\n    plt.legend()\n    plt.savefig(f\"{name}_accuracy_plot.png\", dpi=300)\n    plt.close()\n\n    plt.figure()\n    plt.plot(epochs, grad_norm_hist, label=\"Gradient Norm\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"L2 Norm\")\n    plt.title(f\"{name}: Gradient Norm over Epochs\")\n    plt.legend()\n    plt.savefig(f\"{name}_gradnorm_plot.png\", dpi=300)\n    plt.close()\n\ndef plot_predictions(y_true, y_pred, name):\n    plt.figure(figsize=(12, 5))\n    plt.plot(y_true, label=\"Ground Truth\", linewidth=2)\n    plt.plot(y_pred, label=\"Predictions\", linestyle=\"--\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Normalized Value\")\n    plt.title(f\"{name}: Predictions vs Ground Truth\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(f\"{name}_prediction_plot.png\", dpi=300)\n    plt.close()\n\ndef train_and_eval(model, params_init, X_tr, y_tr, X_te, y_te, name, epochs, lr):\n    params = [p.clone().detach().requires_grad_(True) for p in params_init]\n    opt = torch.optim.AdamW(params, lr=lr)\n    loss_hist, acc_hist, grad_norm_hist = [], [], []\n    param_count = sum(p.numel() for p in params if p.requires_grad)\n    print(f\"Trainable parameters: {param_count}\")    \n    mse_loss = torch.nn.MSELoss()\n    train_ds = TensorDataset(X_tr, y_tr)    \n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, pin_memory=False, num_workers=0)\n    \n\n    with open(f\"{name}_metrics.csv\", \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"epoch\", \"test_loss\", \"test_acc\", \"grad_norm\"])\n\n        for ep in trange(epochs, desc=f\"Training {name}\"):\n        #for ep in range(epochs):\n            for X_batch, y_batch in train_loader:\n                #print(\"X_batch, y_batch shapes\", X_batch.shape, y_batch.shape)\n                opt.zero_grad()                \n                preds = model(X_batch, params)\n                #print(\"preds.shape\", preds.shape)\n                #preds = torch.stack(preds) if isinstance(preds, list) else preds                \n                loss = mse_loss(preds, y_batch)\n                loss.backward()\n                            \n                total_norm = 0.0\n                for p in params:\n                    if p.grad is not None:\n                        param_norm = p.grad.data.norm(2)\n                        total_norm += param_norm.item() ** 2\n                grad_norm = total_norm ** 0.5\n                \n                opt.step()\n\n            with torch.no_grad():                   \n                preds_t = model(X_te, params)\n                loss_t = mse_loss(preds_t, y_te).item()\n                acc_t = prediction_accuracy(preds_t, y_te).item()\n\n            loss_hist.append(loss_t)\n            acc_hist.append(acc_t)\n            grad_norm_hist.append(grad_norm)\n\n            writer.writerow([ep+1, loss_t, acc_t, grad_norm])\n            f.flush()\n\n    plot_metrics(loss_hist, acc_hist, grad_norm_hist, name)\n    torch.save([p.detach() for p in params], f\"{name}_trained_params.pt\")\n\n    with torch.no_grad():\n        #preds_final = torch.stack([model(params, x) for x in X_te])\n        preds_final = model(X_te, params)\n        with open(f\"{name}_predictions.csv\", \"w\", newline=\"\") as f_pred:\n            writer = csv.writer(f_pred)\n            writer.writerow([\"Index\", \"Actual\", \"Predicted\"])\n            for i, (true_val, pred_val) in enumerate(zip(y_te, preds_final)):\n                writer.writerow([i, true_val.item(), pred_val.item()])\n        plot_predictions(y_te.numpy(), preds_final.cpu().numpy(), name)    \n\ndef draw_quantum_circuit():\n    dummy_x = torch.rand((batch_size, lookback, X_features), dtype=torch.float64)\n    \n    dummy_params = [\n        torch.ones((nb_reuploading, nb_qubit_reupload, lookback), dtype=torch.float64) * np.pi,\n        torch.ones((nb_reuploading, nb_qubit_reupload, num_variational), dtype=torch.float64) * np.pi,\n        torch.ones(qml.StronglyEntanglingLayers.shape(n_layers=1, n_wires=nb_qubit_reupload), dtype=torch.float64) * np.pi\n    ]\n\n    # Draw circuit    \n    fig, ax = qml.draw_mpl(quantum_circuit_reupload, decimals=2, style=\"pennylane\")(dummy_x, dummy_params)\n    plt.show()\n\ndata = merged_data['RiverLevel-WaikatoRiver-HamiltonTrafficBr-1stJan1993-23rdMay2025.csv'].iloc[-365:].copy() # Due to limited compute - trainning on last one year of data \nprint(f\"Data shape: {data.shape}\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndraw_quantum_circuit()\nX_tr_np, y_tr_np, X_te_np, y_te_np = split_data(data=data, test_ratio=0.2, lookback=lookback, target_col=target_col)\nX_tr = torch.tensor(X_tr_np, dtype=torch.float64).to(device)\ny_tr = torch.tensor(y_tr_np, dtype=torch.float64).to(device)\nX_te = torch.tensor(X_te_np, dtype=torch.float64).to(device)\ny_te = torch.tensor(y_te_np, dtype=torch.float64).to(device)\n\nent_shape = qml.StronglyEntanglingLayers.shape(n_layers=1, n_wires=nb_qubit_reupload)\n\nparams_init_reupload = [\n    torch.full((nb_reuploading, nb_qubit_reupload, lookback), np.pi, dtype=torch.float64, requires_grad=True).to(device),\n    torch.full((nb_reuploading, nb_qubit_reupload, num_variational), np.pi, dtype=torch.float64, requires_grad=True).to(device),\n    torch.full((ent_shape), np.pi, dtype=torch.float64, requires_grad=True).to(device)\n]\n\ntrain_and_eval(quantum_circuit_reupload, params_init_reupload, X_tr, y_tr, X_te, y_te, \"QRU_ent\", nb_epoch, lr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}